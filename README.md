<h1>
  <img src="https://github.com/user-attachments/assets/c1423d38-458b-412b-9e6b-a74379a01eab" alt="logo" width="100" height="100">
  Flash Sparse Attention (FSA)
</h1>

This repository provides the official implementation of **<ins>F</ins>lash <ins>S</ins>parse <ins>A</ins>ttention (FSA)**, which includes a novel kernel design that enables efficient sparse attention computation across a wide range of popular LLMs on modern GPUs.

* [Features](#features)
* [Installation](#installation)
* [Usage](#usage)
* [Benchmarks](#benchmarks)
* [Citation](#citation)
* [Acknowledgments](#acknowledgments)

## Features

## Installation

## Usage

## Benchmarks

## Citation

## Acknowledgments
