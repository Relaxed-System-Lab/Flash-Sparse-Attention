ninja
transformers
datasets
accelerate
torch>=2.8.0
triton>=3.4.0
# FA v2 for general cases
flash-attn==2.6.3
# FA v3 in hopper platform for general cases
pip install git+https://github.com/Dao-AILab/flash-attention.git#subdirectory=hopper

-r requirements-dev.txt
